{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "In order to pass text input, we need to do some processing? Why? Neural Networks can't process text as is. Therefore we need to convert the text into a mathematical representation (a vector). This mathematical representation is called embeddings. This can be done for other forms of input such as audio and video as well. However, the same embedding tool will not work for all of these. Embeddings can be done at the word, sentence or even paragraph level. Here's a rough outline of the structure we'll follow:\n",
    "1. Tokenize your input text\n",
    "2. Assign a Token ID to the tokens\n",
    "3. Generate embeddings for the tokens.\n",
    "\n",
    "Here is a rough pipeline of what we're building. \n",
    "Img src =  https://geoffrey-geofe.medium.com/tokenization-vs-embedding-understanding-the-differences-and-their-importance-in-nlp-b62718b5964a \n",
    "![alt text](../demo-files/tokenizing_workflow.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing Input Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('demo-files/the-second-shell.txt', <http.client.HTTPMessage at 0x105edc490>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's first get a piece of text that we want to tokenize. Here is the chosen text: https://www.gutenberg.org/cache/epub/73951/pg73951.txt \n",
    "import urllib.request\n",
    "url = (\"https://www.gutenberg.org/cache/epub/73951/pg73951.txt\")\n",
    "file_path = \"demo-files/the-second-shell.txt\"\n",
    "urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was two o'clock in the morning of September 5, 1939. For a year\n",
      "and a half I had been at work on the San Francisco _Times_. I had\n",
      "come there immediately after finishing my year's course at the army\n",
      "officers' flying school at San Antonio, on the chance that my work\n",
      "would lead me into enough tong wars and exciting murder mysteries to\n",
      "make life interesting.\n",
      "\n",
      "The morning edition had just been \"put to bed\" and I was starting out\n",
      "of the office when the night editor called me to meet a visitor who \n"
     ]
    }
   ],
   "source": [
    "# let's take a look at the first 500 characters of the second shell. I modified the text file to get rid of the meta data \n",
    "the_second_shell = open(\"../demo-files/the-second-shell.txt\")\n",
    "print(the_second_shell.read(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../demo-files/the-second-shell.txt', 'r') as f:\n",
    "    input_text = f.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to generate a word level tokenizer, which means I'm going to tokenize my input at the white space character. \n",
    "I'm going to seperate the punctuations from the words and thats about all the processing I'm going to do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'was', 'two', \"o'clock\", 'in', 'the', 'morning', 'of', 'September', '5', ',', '', '1939', '.', '', 'For', 'a', 'year', '\\n', 'and', 'a', 'half', 'I', 'had', 'been', 'at', 'work', 'on', 'the', 'San', 'Francisco', '_Times_', '.', '', 'I', 'had', '\\n', 'come', 'there', 'immediately', 'after', 'finishing', 'my', \"year's\", 'course', 'at', 'the', 'army', '\\n', \"officers'\", 'flying', 'school', 'at', 'San', 'Antonio', ',', '', 'on', 'the', 'chance', 'that', 'my', 'work', '\\n', 'would', 'lead', 'me', 'into', 'enough', 'tong', 'wars', 'and', 'exciting', 'murder', 'mysteries', 'to', '\\n', 'make', 'life', 'interesting', '.', '', '\\n', '', '\\n', 'The', 'morning', 'edition', 'had', 'just', 'been', '\"put', 'to', 'bed\"', 'and', 'I', 'was', 'starting', 'out', '\\n', 'of', 'the', 'office', 'when', 'the', 'night', 'editor', 'called', 'me', 'to', 'meet', 'a', 'visitor', 'who', 'had', '\\n', 'just', 'come', 'in', '.', '', 'The', 'stranger', 'came', 'forward', 'quickly', '.', '', 'Roughly', 'clad', 'in', 'blue', '\\n', 'shirt', 'and', 'overalls', ',', '', 'boots', ',', '', 'and', 'Stetson', ',', '', 'he', 'had', 'the', 'bronze', 'skin', ',', '', 'clear', '\\n', 'eyes', ',', '', 'and', 'smooth', 'movements', 'of', 'one', 'who', 'has', 'spent', 'his', 'life', 'out-of-doors', '.', '', '\\n', '', '\\n', 'He', 'stopped', 'before', 'me', 'and', 'held', 'out', 'his', 'hand', 'with', 'a', 'pleasant', 'smile', '.', '', 'I', 'saw', '\\n', 'that', 'his', 'hair', 'was', 'gray;', 'he', 'was', 'a', 'little', 'older', 'than', 'I', 'had', 'thought', 'at', '\\n', 'first--fifty', ',', '', 'perhaps', '.', '', 'I', 'liked', 'the', 'fellow', 'instinctively', '.', '', '\\n', '', '\\n', '\"Robert', 'Barrett', '?', '\"', 'he', 'questioned', 'in', 'a', 'pleasant', 'drawl', '.', '', 'I', 'nodded', '.', '', '\\n', '', '\\n', '\"I\\'m', 'Bill', 'Johnson', ',', '\"', 'he', 'said', 'briefly', '.', '', '\"I', 'want', 'to', 'see', 'you', '.', '', 'Secret', 'Service', '\\n', 'business', '.', '', '_Sabe', '?', '_\"', 'He', 'let', 'me', 'glimpse', 'a', 'badge', ',', '', 'and', 'we', 'walked', 'out', 'into', '\\n', 'the', 'night', '.', '', 'As', 'we', 'started', 'down', 'the', 'silent', 'street', 'it', 'occurred', 'to', 'me', 'that', '\\n', 'I', 'had', 'heard', 'of', 'this', 'man', 'before', '.', '', '\\n', '', '\\n', '\"Are', 'you', 'the', 'William', 'Johnson', 'who', 'unearthed', 'the', 'radio', 'station', 'of', 'the', '\\n', 'revolutionaries', 'in', 'Mexico', 'in', '1917', '?', '\"', '\\n', '', '\\n', '\"I', 'guess', 'so', '.', '', \"I've\", 'been', 'in', 'Mexico', 'thirty', 'years', ',', '', 'and', \"I've\", 'helped', 'Uncle', '\\n', 'Sam', 'out', 'a', 'time', 'or', 'two', '.', '', \"It's\", 'a', 'case', 'like', 'that', 'one', ',', '', 'or', 'worse', ',', '', 'that', \"I'm\", 'up', '\\n', 'here', 'to', 'see', 'about', 'now', '.', '', 'I', 'need', 'a', 'partner', '.', '', \"I've\", 'been', 'told', 'about', 'you', '.', '', 'Are', '\\n', 'you', 'game', 'for', 'a', 'little', 'adventure', '?', '\"', '\\n', '', '\\n', '\"You\\'ve', 'found', 'your', 'man', '.', '\"', '\\n', '', '\\n', '\"They', 'call', 'you', \"'Tiger\", 'Bob', 'Barrett', ',', \"'\", \"don't\", 'they', '?', '\"', 'he', 'said', 'irrelevantly', '.', '', '\\n', '', '\\n', '\"I', 'used', 'to', 'play', 'football', '.', '\"', '\\n', '', '\\n', 'He', 'laughed', '.', '', 'I', 'have', 'always', 'been', 'sensible', 'about', 'that', 'nickname', '.', '', '\\n', '', '\\n', '\"Well', ',', '', \"here's\", 'the', 'situation', '.', '', \"I've\", 'been', 'at', \"Vernon's\", 'mine', 'in', 'Durango', ',', '', '\\n', 'Mexico', '.', '', 'Called', 'El', 'Tigre', '.', '', 'Gold', 'and', 'thorium', '.', '', \"There's\", 'a', 'little', 'mystery--\"']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pickle\n",
    "\n",
    "def tokenizer(input_text):\n",
    "    # we want to split at white spaces\n",
    "    tokenized_text = re.split(r'([,.?!]|\\s)',input_text)\n",
    "    '''explanation:\n",
    "    We're telling regex to split either on any of the punctations we've provided or on the space character\n",
    "    The return type of this would be a list. \n",
    "    Note that this contains white spaces, so we must get rid of the white spaces as well. \n",
    "    '''\n",
    "    tokenized_text = [token for token in tokenized_text if token != ' ']\n",
    "\n",
    "    # additionally, we notice that the first element of the list has a \\ufeff, which indicates it's a start of the sequence. \n",
    "    # to get rid of this, we can use the following line:\n",
    "    tokenized_text = [token.lstrip('\\ufeff') for token in tokenized_text]\n",
    "\n",
    "    # Generating a file with all the tokens\n",
    "    with open(\"files/tokens.txt\", \"wb\") as file:\n",
    "        pickle.dump(tokenized_text, file)\n",
    "\n",
    "\n",
    "    return tokenized_text\n",
    "\n",
    "print(tokenizer(input_text)[:500]) #printing the first 500 tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
