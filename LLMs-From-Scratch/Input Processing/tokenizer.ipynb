{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "In order to pass text input, we need to do some processing? Why? Neural Networks can't process text as is. Therefore we need to convert the text into a mathematical representation (a vector). This mathematical representation is called embeddings. This can be done for other forms of input such as audio and video as well. However, the same embedding tool will not work for all of these. Embeddings can be done at the word, sentence or even paragraph level. Here's a rough outline of the structure we'll follow:\n",
    "1. Tokenize your input text\n",
    "2. Assign a Token ID to the tokens\n",
    "3. Generate embeddings for the tokens.\n",
    "\n",
    "Here is a rough pipeline of what we're building. \n",
    "Img src =  https://geoffrey-geofe.medium.com/tokenization-vs-embedding-understanding-the-differences-and-their-importance-in-nlp-b62718b5964a \n",
    "![alt text](demo-files/tokenizing_workflow.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing Input Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('demo-files/the-second-shell.txt', <http.client.HTTPMessage at 0x105edc490>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's first get a piece of text that we want to tokenize. Here is the chosen text: https://www.gutenberg.org/cache/epub/73951/pg73951.txt \n",
    "import urllib.request\n",
    "url = (\"https://www.gutenberg.org/cache/epub/73951/pg73951.txt\")\n",
    "file_path = \"demo-files/the-second-shell.txt\"\n",
    "urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'demo-files/the-second-shell.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# let's take a look at the first 500 characters of the second shell. I modified the text file to get rid of the meta data \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m the_second_shell \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdemo-files/the-second-shell.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(the_second_shell\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m500\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'demo-files/the-second-shell.txt'"
     ]
    }
   ],
   "source": [
    "# let's take a look at the first 500 characters of the second shell. I modified the text file to get rid of the meta data \n",
    "the_second_shell = open(\"demo-files/the-second-shell.txt\")\n",
    "print(the_second_shell.read(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"demo-files/the-second-shell.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    input_text = f.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to generate a word level tokenizer, which means I'm going to tokenize my input at the white space character. \n",
    "I'm going to seperate the punctuations from the words and thats about all the processing I'm going to do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 24\u001b[0m\n\u001b[1;32m     19\u001b[0m         file\u001b[38;5;241m.\u001b[39mwrite(tokenized_text)\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_text\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer(input_text)[:\u001b[38;5;241m500\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_text' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def tokenizer(input_text):\n",
    "    # we want to split at white spaces\n",
    "    tokenized_text = re.split(r'([,.?!]|\\s)',input_text)\n",
    "    '''explanation:\n",
    "    We're telling regex to split either on any of the punctations we've provided or on the space character\n",
    "    The return type of this would be a list. \n",
    "    Note that this contains white spaces, so we must get rid of the white spaces as well. \n",
    "    '''\n",
    "    tokenized_text = [token for token in tokenized_text if token != ' ']\n",
    "\n",
    "    # additionally, we notice that the first element of the list has a \\ufeff, which indicates it's a start of the sequence. \n",
    "    # to get rid of this, we can use the following line:\n",
    "    tokenized_text = [token.lstrip('\\ufeff') for token in tokenized_text]\n",
    "\n",
    "    # Generating a file with all the tokens\n",
    "    with open(\"./files/tokens.txt\", \"w\") as file:\n",
    "    # Write some text to the file\n",
    "        file.write(tokenized_text)\n",
    "\n",
    "\n",
    "    return tokenized_text\n",
    "\n",
    "print(tokenizer(input_text)[:500]) #printing the first 500 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
